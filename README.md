# NLP AI Writing Detection Project :
## Introduction and Project Overview:
With the rise and accessibility to ChatGPT, people can use generative AI for a variety of tasks, such as writing messages or essays. This helps save a user’s time and creative mental effort in these tasks, but it has also led to the rise of potential plagiarism and become an obstacle in having inexperienced writers improve their writing skills due to their increased dependency on AI tools. Despite their sophistication, generative AI models still exhibit subtle differences in their use of language compared to humans. The linguistic characteristic differences between human-written versus generative AI texts are potential useful features when creating a classifier to conduct AI detection, which can combat potential plagiarism, help education systems continue to teach students how to improve their writing skills by reducing their dependency on AI tools, and check authenticity in future written works. Thus, there is a strong need to create an AI detection classifier to ensure that essays are truly sourced from the original writer and to continue instilling the critical writing skills needed in future generations.

The NLP AI Writing Detection Project focuses on identifying whether there is a linguistic characteristic difference with average semantic similarity between words, which indicates whether a particular text stays on-topic to a greater degree compared to other texts, among AI-generated and human-written essays. By identifying whether there is a linguistic characteristic difference with average semantic similarity between words for AI-generated and human-written essays, we can determine whether this linguistic characteristic would be suitable in a future AI detection classifier.

## File Folder Structure:
All of our files can be found in the following file folder structure:
- code : Folder contains the code used to conduct the study. The folder contains a Jupyter notebook and a PDF version of the study's code. The code in this folder reflects additional edits made to Tiffany's DS 5780's data assignment 2
- data : Folder contains all of the data used in this study. The data found in this folder is made publicly available as noted in the Data section of the ReadMe. 
- final_report : Folder contains the unpublished Educational Data Mining manuscript describing the process and results from this study in-depth. The PDF version of the manuscript is in this folder. 

## Data
Two datasets are used to conduct the analysis:
1. **LLM - Detect AI Generated Text Kaggle Competition**
   - Dataset contains 1378 essays where 1375 essays are human-written essay text.
   - Human-written essays are from middle and high school students where students were instructed to read the prompt and various source texts before writing a response.
   - Dataset posted on the Kaggle competition is from Vanderbilt University's Peabody College and the Learning Agency Lab. This dataset was made possible with the support from the Bill & Melinda Gates Foundation, Schmidt Futures, and Chan Zuckerberg Initiative.
   - Link: [https://www.kaggle.com/competitions/llm-detect-ai-generated-text/overview](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/overview)
2. **LLM Generated Essays for the Detect AI Competition**
   - Dataset generated by Radek Osmulski competing in the above Kaggle competition.
   - Dataset contains 700 AI-generated essays based on the same prompts used in the Kaggle competition.
     - 500 essays are from GPT 3.5 Turbo and 200 are from GPT 4.
   - Link: [https://www.kaggle.com/datasets/radek1/llm-generated-essays/data](https://www.kaggle.com/datasets/radek1/llm-generated-essays/data)

All essays in this dataset were written in response to one of two prompts and more details on the prompts can be found in the data folder:

#### **Car-free Cities Prompt:**
> "Write an explanatory essay to inform fellow citizens about the advantages of limiting car usage. Your essay must be based on ideas and information that can be found in the passage set. Manage your time carefully so that you can read the passages; plan your response; write your response; and revise and edit your response. Be sure to use evidence from multiple sources; and avoid overly relying on one source. Your response should be in the form of a multiparagraph essay. Write your essay in the space provided."

**-or-**

#### **Electoral College Prompt:**
> "Write a letter to your state senator in which you argue in favor of keeping the Electoral College or changing to election by popular vote for the president of the United States. Use the information from the texts in your essay. Manage your time carefully so that you can read the passages; plan your response; write your response; and revise and edit your response. Be sure to include a claim; address counterclaims; use evidence from multiple sources; and avoid overly relying on one source. Your response should be in the form of a multiparagraph essay. Write your response in the space provided."

After students read their respective prompts, they were instructed to read one or more provided source texts before writing a response. For AI-generated essays, AI was also given the same prompt and same source texts before providing a written response. Both datasets are put together to create the finalized dataset for analysis.

The finalized dataset will contain the following columns:
- **id** – Essay_id 
- **prompt_id** – a binary value of 0 or 1 for the essay’s prompt
  - 0 - Car-Free Cities
  - 1 – Electoral College
- **text** - Essay text
- **generated** – identifies specifically how the essay was created
  - 0 – Human-written
  - 1 – GPT 3.5 Turbo
  - 2 – GPT 4
- **is_ai** – a binary value of 0 or 1 to identify whether generative AI was used to create the essay
  - 0 – Human-written
  - 1 – Generative AI used
- **prompt_and_is_ai** – combines values from prompt_id and is_ai to create the following categories:
  - Car-Free_and_Human
  - Car-Free_and_AI
  - Electoral_and_Human
  - Electoral_and_AI

The outcome variable is the *is_ai* column as that determines whether an essay is from generative AI or human-written.

## Statistical Analysis Process:
Two types of statistical analysis were conducted in this study:
- Independent T-Tests - this statistical test was applied to both the general AI vs. human-written group as a whole and in the post-hoc analysis when conducting whether the average semantic similarity between words is a linguistic characteristic difference when accounting for the specific prompt sub-groups.
- Multiple Linear Regression Model - this statistical test was applied only in the post-hoc section to create a regression model with both authorship and prompt predictors.
  - We end up with a regression equation as: average semantic similarity between words score = 0.2264 + (0.0716 * is_ai) + (0.0170 * prompt_id)
    - is_ai and prompt_id are binary values

## Results:
Regarding the AI vs. Human General Comparison category, we first saw that there is a statistically significant difference in average semantic similarity scores between words where AI-generated texts will have higher average scores indicating that an AI-generated text is more on-topic compared to human-written essays in general without accounting for the prompt essay sub-groups. Based on the evidence found in the difference in average semantic similarity scores between words for AI generated essays to human-written essays in general, we also explored in the post-hoc analysis whether there is statistically significant evidence when accounting for the different prompt essay groups in the AI vs. Human Prompt Split General Comparison. We wanted to explore the same analysis after accounting for different prompts because prompts may inherently lead to more or less semantic similarity between words for each text.

When doing the t-test for both "Car-Free Cities" and "Electoral College" prompts, we saw that there is a difference in the average semantic similarity scores between words for AI-generated and human-written essays. This implies that the results found in AI vs. Human general comparison section is consistent across the two different prompts in our dataset. To also mitigate the potential type I error due to family-wise error rate that would need to have a correction, we performed a multiple linear regression analysis where the results are consistent to what we had seen in the earlier t-tests and how there is little interaction effect between our predictor variables. Thus, the results from conducting the average semantic similarity between words in AI-generated and human-written texts for both AI-generated and human-written texts in general and across different prompts suggests that the difference in average semantic similarity scores between words is inherent to how AI and human convey written text. The results from this analysis further aligns with other previous research studies that also identified that semantics is a linguistic characteristic difference between human-written versus generative AI essays. As a result, we can consider using average semantic similarity between words score as a feature for AI detection to hopefully counter plagiarism and continue supporting written text from human thought.

## Future Analysis Considerations:
Given the analysis conducted so far, there are additional future considerations for further research:
- **Conduct a cross-analysis using different groupings:**
  - Split the data based on their generative AI model to see a deeper analysis on what subtle written text differences are associated with each generative AI model compared to human-written text. This cross-analysis can also be done on the prompts as well.
- **Calculate semantic similarity score between sentences:**
  - In this analysis, we focused specifically on calculating the semantic similarity between words. According to other research studies, there is an interest in calculating a text's average cohesion score, which is the semantic similarity score between sentences. The cohesion metric would require extrapolating the code previously found in this paper. By exploring cohesion between human-written versus generative AI essays, we can evaluate whether it is also a linguistic characteristic difference that can be used as a feature in our future AI detection classifier.
- **Use different prompts:**
  - According to the human-written essays dataset, there are additional prompts given to students to write that are not included in the public dataset. A future consideration is to acquire the additional prompts to see if there are any result differences to the prompt analysis conducted in this paper.

## Contact:
If you have any questions or issues, please feel free to reach out to Tiffany Lee:
- **Tiffany Lee**
  - Email: Tiffany.Lee@vanderbilt.edu
  - GitHub: Math1019
